{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "205e095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize, wordpunct_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64c333a",
   "metadata": {},
   "source": [
    "# 1. Сбор данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61ddc40",
   "metadata": {},
   "source": [
    "Собрать данные со всех файлов: имена героев и их реплики. Файлы все собраны в одной папке, в каждом файле имя героя, потом двоеточие и после реплика. Кроме это были лишние данные в виде описания сцены и действий персонажей, которые я удалила"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7dc926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_character = re.compile(r'(.+?):')  # найти имя персонажа\n",
    "re_text = re.compile(r': (.+)')  # найти реплику\n",
    "re_episode = re.compile(r'(\\w+?)\\.')  # выделить название эпизода = название файла\n",
    "re_del = re.compile(r'\\[.+?\\]')  # удаление ненужных данных\n",
    "re_title = re.compile(r'[A-Z][a-z]*')  # разделение название эпизода пробелами, так как слова написаны слитно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bf4445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_character(lines, file):\n",
    "    character_texts = []\n",
    "    episode = re_episode.match(file)[1]\n",
    "    for n, line in enumerate(lines):\n",
    "        line = re_del.sub('', line).replace('\\xa0', ' ')\n",
    "        if re_character.match(line) and re_text.search(line):\n",
    "            character = re_character.match(line)[1]\n",
    "            text = re_text.search(line)[1]\n",
    "            character_texts.append(\n",
    "                {'character': character, \n",
    "                 'episode': episode, \n",
    "                 'line': text}\n",
    "            )\n",
    "    return character_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1020fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 393/393 [00:00<00:00, 702.48it/s]\n"
     ]
    }
   ],
   "source": [
    "characters_table = pd.DataFrame(columns=['character', 'episode', 'line'])\n",
    "for root, dirs, files in os.walk('../SpongeBob_SquarePants_Transcripts'):\n",
    "    for file in tqdm(files):\n",
    "        path = os.path.join(root, file)\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        character_texts = define_character(lines, file)\n",
    "        characters_table = characters_table.append(character_texts, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3789a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_table = characters_table.drop(index=32911)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3adb9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_title(title):\n",
    "    return ' '.join(re_title.findall(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62792cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_table['episode'] = characters_table['episode'].apply(normalize_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82cc736",
   "metadata": {},
   "source": [
    "# 2. Анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d6bb3",
   "metadata": {},
   "source": [
    "Какие поинты можно отметить:\n",
    "- длина реплики каждого персонажа из основных\n",
    "- самое часто слово у каждого / во всём сериале\n",
    "- серия с наибольшим количеством реплик\n",
    "- облака слов каждого персонажа из основных\n",
    "- можно для прикола просто факты накидать (спарсить с сайта какого-нибудь?:))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e729e1c",
   "metadata": {},
   "source": [
    "Я выделила 8 основных персонажей мультсериала, создала словарь, куда записывала полученные данные по каждому персонажу, а также создала отдельную таблицу, чтобы анализировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5519e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_info = {'SpongeBob': {},\n",
    "            'Patrick': {},\n",
    "            'Mr. Krabs': {},\n",
    "            'Squidward': {},\n",
    "            'Plankton': {},\n",
    "            'Sandy': {},\n",
    "            'Karen': {},\n",
    "            'Gary': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81f6bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_char = ['SpongeBob', 'Patrick', 'Mr. Krabs', 'Squidward', 'Plankton', 'Sandy', 'Karen', 'Gary']\n",
    "main_char_table = characters_table.copy()[characters_table['character'].isin(main_char)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f4bf8",
   "metadata": {},
   "source": [
    "### Длина реплики"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7e7df",
   "metadata": {},
   "source": [
    "Я посчитала медианную длину реплики каждого их основных персонажей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cfb07898",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_char_table['len_line'] = main_char_table['line'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b0d35172",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnt_len_table = (main_char_table\n",
    "                 .groupby('character')['len_line']\n",
    "                 .agg('median')\n",
    "                 .reset_index(name='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d9c81a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in cnt_len_table.iterrows():\n",
    "    char_info[row['character']]['length'] = row['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539c1e1",
   "metadata": {},
   "source": [
    "### Вордклауды"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fee9e30",
   "metadata": {},
   "source": [
    "Для каждого из основных персонажей я создала облако слов. \n",
    "\n",
    "Для этого понадобилось:\n",
    "1. С помощью модуля nltk очистить и лемматизировать текст\n",
    "2. Найти маски для каждого из персонажей, чтобы сделать фигурные облака слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c43d601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = main_char_table.groupby('character')['line'].agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71a10134",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_sw = ['go', 'get', 'oh', \n",
    "            'well', 'like', 'come', \n",
    "            'look', 'know', 'see', \n",
    "            'hey', 'na', 'one', 'two']\n",
    "sw = stopwords.words('english') + found_sw\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# для корректной работы лемматизатора понадобилось перевести\n",
    "# выражение части речи, которая получалась в результате функции pos_tag\n",
    "# в выражение части речи, которую принимала на вход функция lemmatize\n",
    "nltk_to_pos = {'ADV': 'r', 'NOUN': 'n', 'VERB': 'v', 'ADJ': 'a'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d71ae1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_model(lines):\n",
    "    lemmas = []\n",
    "    new_sent = []\n",
    "    for line in lines:\n",
    "        sentences = sent_tokenize(line)\n",
    "        for sent in sentences:\n",
    "            words = [word for word in word_tokenize(sent) if word.isalpha()]\n",
    "            for word in words:\n",
    "                pos = pos_tag([word], tagset='universal')[0][1]\n",
    "                if pos in ['VERB', 'NOUN', 'ADV', 'ADJ']:\n",
    "                    word = lemmatizer.lemmatize(word.lower(), pos=nltk_to_pos[pos])\n",
    "                else:\n",
    "                    word = lemmatizer.lemmatize(word.lower())\n",
    "                if word not in sw:\n",
    "                    lemmas.append(word)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09c2e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines['lemmas'] = all_lines['line'].apply(for_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c15ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = {'SpongeBob': '../masks/spongebob_mask.png', \n",
    "         'Patrick': '../masks/patrick_mask.png',\n",
    "         'Mr. Krabs': '../masks/mrkrabs_mask.png',\n",
    "         'Sandy': '../masks/sandy_mask.png',\n",
    "         'Squidward': '../masks/squidward_mask.png',\n",
    "         'Karen': '../masks/karen_mask.png',\n",
    "         'Plankton': '../masks/plankton_mask.png',\n",
    "         'Gary': '../masks/gary_mask.png'\n",
    "        }\n",
    "for i, name in enumerate(all_lines['character'].unique()):\n",
    "    mask = np.array(Image.open(masks[name]))\n",
    "    text = ' '.join(all_lines[all_lines['character'] == name]['lemmas'][i])\n",
    "    wordcloud = WordCloud(\n",
    "        background_color ='white',\n",
    "        width = 800,\n",
    "        height = 800,\n",
    "        mask = mask).generate(text)\n",
    "    wordcloud.to_file(f'{masks[name]}_cloud.png')\n",
    "    char_info[name]['wordcloud'] = f'{masks[name][1:]}_cloud.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da89d2",
   "metadata": {},
   "source": [
    "### Наиболее частые слова персонажа"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10ce212",
   "metadata": {},
   "source": [
    "Для каждого из основных персонажей я выделила топ-5 слов, которые они используют. Неудивительно, что у каждого из них (кроме Гэри, сответственно) одно из топ-5 слов - это SpongeBob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d371d9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequent_word(words):\n",
    "    count_words = Counter(words)\n",
    "    return count_words.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8c804b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines['frequent'] = all_lines['lemmas'].apply(frequent_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8efd6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in all_lines.iterrows():\n",
    "    char_info[row['character']]['most_words'] = row['frequent']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e39a55",
   "metadata": {},
   "source": [
    "### Количество серий, в которых встречается персонаж"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50140a2b",
   "metadata": {},
   "source": [
    "Последнее, что я выяснила про персонажа, это количество эпизодов, в которых он встретился"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5664b54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_episodes(episodes):\n",
    "    return len(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03c31b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_episodes = main_char_table.groupby('character')['episode'].unique().reset_index(name='all_episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16f8494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_episodes['count_episodes'] = all_episodes['all_episodes'].apply(count_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7401a05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character</th>\n",
       "      <th>all_episodes</th>\n",
       "      <th>count_episodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Karen</td>\n",
       "      <td>[Karen, A Cabininthe Kelp, Goo Goo Gas, Grandm...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sandy</td>\n",
       "      <td>[Oral Report, Stuckonthe Roof, A Cabininthe Ke...</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Plankton</td>\n",
       "      <td>[The Krusty Slammer, The Great Patty Caper, Ka...</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gary</td>\n",
       "      <td>[Penny Foolish, Sentimental Sponge, Bummer Vac...</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mr. Krabs</td>\n",
       "      <td>[Lighthouse Louie, Penny Foolish, Barnacle Fac...</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patrick</td>\n",
       "      <td>[Oral Report, Sentimental Sponge, Fun Sized Fr...</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Squidward</td>\n",
       "      <td>[Penny Foolish, Oral Report, The Krusty Slamme...</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SpongeBob</td>\n",
       "      <td>[Lighthouse Louie, Penny Foolish, Barnacle Fac...</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   character                                       all_episodes  \\\n",
       "1      Karen  [Karen, A Cabininthe Kelp, Goo Goo Gas, Grandm...   \n",
       "5      Sandy  [Oral Report, Stuckonthe Roof, A Cabininthe Ke...   \n",
       "4   Plankton  [The Krusty Slammer, The Great Patty Caper, Ka...   \n",
       "0       Gary  [Penny Foolish, Sentimental Sponge, Bummer Vac...   \n",
       "2  Mr. Krabs  [Lighthouse Louie, Penny Foolish, Barnacle Fac...   \n",
       "3    Patrick  [Oral Report, Sentimental Sponge, Fun Sized Fr...   \n",
       "7  Squidward  [Penny Foolish, Oral Report, The Krusty Slamme...   \n",
       "6  SpongeBob  [Lighthouse Louie, Penny Foolish, Barnacle Fac...   \n",
       "\n",
       "   count_episodes  \n",
       "1              55  \n",
       "5              96  \n",
       "4              97  \n",
       "0             110  \n",
       "2             262  \n",
       "3             270  \n",
       "7             295  \n",
       "6             389  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_episodes.sort_values(['count_episodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c5b6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in all_episodes.iterrows():\n",
    "    char_info[row['character']]['count_episodes'] = row['count_episodes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "48ffa249",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('char_info.json', 'w') as f:\n",
    "    f.write(json.dumps(char_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f51860",
   "metadata": {},
   "source": [
    "### Количество реплик в эпизоде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c4fda4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_per_episode = (characters_table\n",
    "                     .groupby('episode')['line']\n",
    "                     .agg('count')\n",
    "                     .reset_index(name='lines_cnt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7aeb2",
   "metadata": {},
   "source": [
    "### Количество персонажей в эпизоде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "34680de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_per_episode = (characters_table\n",
    "                     .groupby('episode')['character']\n",
    "                     .agg('unique')\n",
    "                     .reset_index(name='unique_char'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "69abc0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_per_episode['unique_char_cnt'] = character_per_episode['unique_char'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6188012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_stat = {'lines_cnt': {}, 'chatacters_cnt': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "66ddee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = character_per_episode.join(lines_per_episode.set_index('episode'), on='episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "50974085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# больше всего персонажей\n",
    "a = episodes.sort_values(['unique_char_cnt'], ascending=False)[:3]\n",
    "# меньше всего персонажей\n",
    "b = episodes.sort_values(['unique_char_cnt'], ascending=True)[:3]\n",
    "for i, row in a.iterrows():\n",
    "    episode_stat['chatacters_cnt'][row['episode']] = [row['unique_char_cnt']]\n",
    "for i, row in b.iterrows():\n",
    "    episode_stat['chatacters_cnt'][row['episode']] = [row['unique_char_cnt'], row['unique_char'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44f0e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# больше всего реплик\n",
    "a = episodes.sort_values(['lines_cnt'], ascending=False)[:3]\n",
    "# меньше всего реплик\n",
    "b = episodes.sort_values(['lines_cnt'], ascending=True)[:3]\n",
    "for i, row in a.iterrows():\n",
    "    episode_stat['lines_cnt'][row['episode']] = row['lines_cnt']\n",
    "for i, row in b.iterrows():\n",
    "    episode_stat['lines_cnt'][row['episode']] = row['lines_cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1cd125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('episodes.json', 'w') as f:\n",
    "    f.write(json.dumps(episode_stat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
